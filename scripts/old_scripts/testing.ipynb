{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b04b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dagc.graphs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m ROOT = Path(\u001b[33m\"\u001b[39m\u001b[33m/Users/nicholaskhorasani/Documents/CodingProjects/diffusion-aware-graph-compression\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(ROOT / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdagc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_graph, generate_seed_set\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Experiment configuration\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m graph_path = ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mgraphs\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mba_1000_3.edgelist\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dagc.graphs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Make sure we can import the dagc package from src/\n",
    "ROOT = Path(\"/Users/nicholaskhorasani/Documents/CodingProjects/diffusion-aware-graph-compression\")\n",
    "sys.path.append(str(ROOT / \"src\"))\n",
    "\n",
    "from dagc.graphs import read_graph, generate_seed_set\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment configuration\n",
    "# -----------------------------\n",
    "graph_path = ROOT / \"data\" / \"graphs\" / \"ba_1000_3.edgelist\"\n",
    "\n",
    "num_runs = 100\n",
    "seed_set_size = 5\n",
    "activation_prob = 0.1\n",
    "keep_ratio = 0.5  # keep 50% of edges â†’ remove 50%\n",
    "base_seed = 12345\n",
    "\n",
    "# Path to a trained GNN checkpoint (to be produced by your training script)\n",
    "model_checkpoint = ROOT / \"checkpoints\" / \"gnn_sparsifier.pt\"\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "print(\"=== GNN Edge Sparsification Experiment ===\")\n",
    "print(f\"Graph path:       {graph_path}\")\n",
    "print(f\"Num runs:         {num_runs}\")\n",
    "print(f\"Seed set size:    {seed_set_size}\")\n",
    "print(f\"Activation prob:  {activation_prob}\")\n",
    "print(f\"Keep ratio:       {keep_ratio}\")\n",
    "print(f\"Base RNG seed:    {base_seed}\")\n",
    "print(f\"Model checkpoint: {model_checkpoint}\")\n",
    "print(f\"Device:           {device}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# Load original graph\n",
    "# -----------------------------\n",
    "print(\"Loading original graph...\")\n",
    "G = read_graph(str(graph_path), fmt=\"edge_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10407d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = sorted(G.nodes())              # e.g. [0,1,2,...] but could be arbitrary\n",
    "node_to_idx = {u: i for i, u in enumerate(nodes)}\n",
    "n = len(nodes)\n",
    "\n",
    "src = []\n",
    "dst = []\n",
    "for u, v in G.edges():\n",
    "    i = node_to_idx[u]\n",
    "    j = node_to_idx[v]\n",
    "    # if undirected, we store both directions for message passing\n",
    "    src.append(i); dst.append(j)\n",
    "    src.append(j); dst.append(i)\n",
    "\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)  # shape: [2, num_edges_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9071c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   0,  ..., 885, 896, 937],\n",
       "        [  1,   0,   2,  ..., 913, 937, 896]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "degrees = torch.tensor(\n",
    "    [G.degree[u] for u in nodes],\n",
    "    dtype=torch.float32,\n",
    ").unsqueeze(-1)  # [n, 1]\n",
    "\n",
    "ones = torch.ones_like(degrees)        # [n, 1]\n",
    "\n",
    "x = torch.cat([degrees, ones], dim=1)  # [n, 2] â†’ in_dim = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac47a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degrees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52.,  1.],\n",
       "        [31.,  1.],\n",
       "        [ 4.,  1.],\n",
       "        ...,\n",
       "        [ 3.,  1.],\n",
       "        [ 3.,  1.],\n",
       "        [ 3.,  1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0130a",
   "metadata": {},
   "source": [
    "# ðŸ§  Understanding the Forward Pass of `SimpleGCNLayer`\n",
    "\n",
    "These are my notes on *exactly* what happens inside one GCN layer â€” how node features propagate, what `x_src` is doing, how `index_add_` works, what shapes everything has, and how the learnable weights get applied.  \n",
    "This is written in my own tone so I fully understand every step.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Objects and Shapes\n",
    "\n",
    "We start with:\n",
    "\n",
    "- **x**: node feature matrix  \n",
    "  Shape: `[num_nodes, in_dim]`  \n",
    "  Example: `[1000, 2]` â†’ 1000 nodes with 2 features (degree + constant 1)\n",
    "\n",
    "- **edge_index**  \n",
    "  Shape: `[2, num_edges_dir]`  \n",
    "  Each column is `(src, dst)` for a directed edge.\n",
    "\n",
    "Because the graph is undirected, we manually store **both directions**:\n",
    "```\n",
    "(u â†’ v), (v â†’ u)\n",
    "```\n",
    "\n",
    "- **src**: `[num_edges_dir]`  \n",
    "- **dst**: `[num_edges_dir]`\n",
    "\n",
    "Our GCN layer implements:\n",
    "```\n",
    "h_new(v) = ReLU( W_self * h(v)  +  W_neigh * mean_{u in N(v)} h(u) )\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `h(v)` is the current node feature\n",
    "- `N(v)` are vâ€™s neighbors\n",
    "- `W_self`, `W_neigh` are shared learnable matrices\n",
    "\n",
    "---\n",
    "\n",
    "# 2. What Are h(v), h(u), and N(v)?\n",
    "\n",
    "- **h(v)** â€” the feature vector for node v at the current layer (initially `x[v]`)\n",
    "- **h(u)** â€” same thing, but for neighbor u\n",
    "- **N(v)** â€” neighbors of v\n",
    "\n",
    "Important:\n",
    "> Each node **does NOT have its own weights**.  \n",
    "> All nodes use the *same* W_self and W_neigh.\n",
    "\n",
    "This is what makes GNNs generalize.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Computing `x_src` â€” messages from source nodes\n",
    "\n",
    "Given:\n",
    "```\n",
    "src = [s0, s1, s2, ..., sK]\n",
    "```\n",
    "\n",
    "We gather source-node features:\n",
    "\n",
    "```\n",
    "x_src = x[src]          # shape: [num_edges_dir, in_dim]\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "- `x_src[i] = x[src[i]]`\n",
    "- If edge i is `0 â†’ 1`, `x_src[i] = x[0]`.\n",
    "\n",
    "So:\n",
    "> `x_src` is the â€œmessageâ€ attached to every edge, copied from the node where the edge *starts*.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Understanding `index_add_`: summing messages into destination nodes\n",
    "\n",
    "We want:\n",
    "```\n",
    "neigh_agg[v] = sum of all x_src[i] where dst[i] = v\n",
    "```\n",
    "\n",
    "Initialize:\n",
    "```\n",
    "neigh_agg = zeros_like(x)     # shape: [num_nodes, in_dim]\n",
    "```\n",
    "\n",
    "Then:\n",
    "```\n",
    "neigh_agg.index_add_(0, dst, x_src)\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "For each edge i:\n",
    "```\n",
    "node = dst[i]\n",
    "neigh_agg[node] += x_src[i]\n",
    "```\n",
    "\n",
    "So:\n",
    "> neigh_agg[v] becomes the sum of all incoming neighbor feature vectors to v.\n",
    "\n",
    "This matches exactly what I expected:\n",
    "- For node v, we add up all feature vectors of nodes that point to v.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Compute Degrees Using the Same Trick\n",
    "\n",
    "We count how many edges go into each node:\n",
    "\n",
    "```\n",
    "deg = torch.zeros(num_nodes)\n",
    "deg.index_add_(0, dst, torch.ones_like(dst))\n",
    "```\n",
    "\n",
    "This does:\n",
    "```\n",
    "deg[v] = number of incoming edges to v\n",
    "```\n",
    "\n",
    "Then:\n",
    "```\n",
    "deg = deg.clamp(min=1).unsqueeze(-1)\n",
    "```\n",
    "so we avoid dividing by zero.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Mean Neighbor Features\n",
    "\n",
    "Divide neighbor sums by degree:\n",
    "\n",
    "```\n",
    "neigh_mean = neigh_agg / deg     # shape: [num_nodes, in_dim]\n",
    "```\n",
    "\n",
    "So:\n",
    "> neigh_mean[v] is literally the average of all neighbor features of node v.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. What Are W_self and W_neigh?\n",
    "\n",
    "These are **matrices** created by:\n",
    "\n",
    "```\n",
    "self.W_self  = nn.Linear(in_dim, out_dim, bias=True)\n",
    "self.W_neigh = nn.Linear(in_dim, out_dim, bias=False)\n",
    "```\n",
    "\n",
    "If:\n",
    "- `in_dim = 2`\n",
    "- `out_dim = 64`\n",
    "\n",
    "Then:\n",
    "\n",
    "- `W_self.weight`: `[64, 2]`  \n",
    "- `W_self.bias`: `[64]`  \n",
    "- `W_neigh.weight`: `[64, 2]`  \n",
    "\n",
    "These are the parameters the model will **learn**.\n",
    "\n",
    "---\n",
    "\n",
    "# 8. What Happens Inside `W_self(x)`?\n",
    "\n",
    "PyTorch performs:\n",
    "```\n",
    "W_self(x)  =  x @ W_self.weight^T  +  bias\n",
    "```\n",
    "\n",
    "Shapes:\n",
    "- x: `[num_nodes, 2]`\n",
    "- W_self.weight: `[64, 2]`\n",
    "- result: `[num_nodes, 64]`\n",
    "\n",
    "Same for:\n",
    "```\n",
    "W_neigh(neigh_mean)\n",
    "```\n",
    "\n",
    "Thus:\n",
    "- each nodeâ€™s own 2D vector becomes 64D\n",
    "- each nodeâ€™s neighbor-mean 2D vector also becomes 64D\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Combining Everything\n",
    "\n",
    "We compute:\n",
    "\n",
    "```\n",
    "out = W_self(x) + W_neigh(neigh_mean)\n",
    "out = ReLU(out)\n",
    "```\n",
    "\n",
    "So for **each** node v:\n",
    "\n",
    "```\n",
    "h_self  = W_self * h(v)\n",
    "h_neigh = W_neigh * mean_neighbor_features(v)\n",
    "h_new(v) = ReLU( h_self + h_neigh )\n",
    "```\n",
    "\n",
    "The result:\n",
    "- shape: `[num_nodes, out_dim]`  \n",
    "- typically `[num_nodes, 64]`\n",
    "\n",
    "This becomes the input to the next GCN layer.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Final Summary (my own words)\n",
    "\n",
    "- Each node starts with simple features (like degree + 1).\n",
    "- We copy each source-node's feature onto every outgoing edge â†’ `x_src`.\n",
    "- We sum these for each destination node using `index_add_` â†’ `neigh_agg`.\n",
    "- We divide by degree â†’ `neigh_mean`.\n",
    "- We apply **shared** learnable matrices:\n",
    "  - `W_self` on the nodeâ€™s own feature,\n",
    "  - `W_neigh` on the mean neighbor feature.\n",
    "- We add them and apply ReLU.\n",
    "- This gives a new embedding per node (e.g. 64D), which feeds into the next layer.\n",
    "\n",
    "These parameters update during training so that edge scores (produced later by an MLP) lead to a compressed graph that preserves diffusion behavior.\n",
    "\n",
    "This is the entire forward pass, line by line, and now I understand every piece of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227a78c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m F.relu(out)\n\u001b[32m     48\u001b[39m network = SimpleGCNLayer(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m network.forward(\u001b[43mx\u001b[49m, edge_index)\n",
      "\u001b[31mNameError\u001b[39m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGCNLayer(nn.Module):\n",
    "    \"\"\"This is a very simple GCN-like Layer. Graph convolutional neural network\n",
    "    h_new(v) = ReLU(W-self h(v) + W_neigh * mean_{u in N(v)} h(u))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.W_self = nn.Linear(in_dim, out_dim, bias = True)\n",
    "        self.W_neigh = nn.Linear(in_dim, out_dim, bias = False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x : [num_nodes, in_dim]\n",
    "        edge_index: [2, num_edges_dir] with (src, dst) per column\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        src, dst = edge_index\n",
    "        \n",
    "        # Gather source node embeddings for each edge\n",
    "        # Basically this is the \"node features\" of the \"source node\" of each edge. \n",
    "        # So the 0th element of x_src will be the node features of the source node of the 0th edge\n",
    "        # this is probably the node features of node 0!\n",
    "        x_src = x[src] #[num_edges_dir, in_dim]\n",
    "        \n",
    "        # Initialize neighbour aggregation with zeros\n",
    "        neigh_agg = torch.zeros_like(x) # [num_nodes, in_dim]\n",
    "        # neigh_agg[v] will store the sum of features received from all neighbors of v.\n",
    "        # Aggregate (sum) messages into destination nodes\n",
    "        neigh_agg.index_add_(0, dst, x_src) # Sum messages into neigh_add[dst]\n",
    "        \n",
    "        # Compute degree for normalization (numbwer of incoming edges)\n",
    "        deg = torch.zeros(num_nodes, device = x.device, dtype = x.dtype)\n",
    "        one_vec = torch.ones_like(src, dtype = x.dtype)\n",
    "        deg.index_add_(0, dst, one_vec)\n",
    "        # Avoid division by zero\n",
    "        deg = torch.clamp(deg, min= 1.0).unsqueeze(-1) # [num_nodes, 1]\n",
    "        \n",
    "        neigh_mean = neigh_agg / deg # Mean Neighbor features\n",
    "        \n",
    "        out = self.W_self(x) + self.W_neigh(neigh_mean)\n",
    "        return F.relu(out)\n",
    "network = SimpleGCNLayer(2, 2)\n",
    "network.forward(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of simple GCNLayers\n",
    "    h^(0) = x\n",
    "    h^(l+1) = SimpleGCNLayer_l(h^(l), edge_index)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # First layer: in_dim -> hidden_dim\n",
    "        layers.append(SimpleGCNLayer(in_dim, hidden_dim))\n",
    "        \n",
    "        #Remaining layers hidden_dim -> hidden_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(SimpleGCNLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [num_nodes, in_dim] (in_dim is the dimension of the features for node i)\n",
    "        edge_index: [2, num_edges_dir]\n",
    "        returns: H [num_nodes, hidden_dim] \n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b476cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4])\n"
     ]
    }
   ],
   "source": [
    "encoder = GNNEncoder(in_dim=2, hidden_dim=4, num_layers=2)\n",
    "H = encoder(x, edge_index)\n",
    "print(H.shape)  # should be [num_nodes, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43df64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNSparsifierOutput:\n",
    "    node_embeddings: torch.Tensor   # [num_nodes, hidden_dim]\n",
    "    edge_logits: torch.Tensor       # [num_edges_dir]\n",
    "    edge_probs: torch.Tensor        # [num_edges_dir]\n",
    "\n",
    "\n",
    "class GNNSparsifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        edge_mlp_hidden_dim: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(\n",
    "            in_dim=in_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        # Edge feature dimension: [h_u, h_v, h_u * h_v, |h_u - h_v|]\n",
    "        edge_feat_dim = 4 * hidden_dim\n",
    "        \n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_feat_dim, edge_mlp_hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(edge_mlp_hidden_dim, 1),  # scalar logit per edge\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,          # [num_nodes, in_dim]\n",
    "        edge_index: torch.Tensor, # [2, num_edges_dir]\n",
    "    ) -> GNNSparsifierOutput:\n",
    "        # Step 1) Node embeddings from GNN\n",
    "        H = self.encoder(x, edge_index)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Step 2) Build per-edge features \n",
    "        src, dst = edge_index\n",
    "        \n",
    "        # Learned embedding for the source / dest node of each edge\n",
    "        h_src = H[src]  # [num_edges_dir, hidden_dim]\n",
    "        h_dst = H[dst]  # [num_edges_dir, hidden_dim]\n",
    "        \n",
    "        edge_feat = torch.cat(\n",
    "            [h_src, h_dst, h_src * h_dst, torch.abs(h_src - h_dst)],\n",
    "            dim=-1,\n",
    "        )  # [num_edges_dir, 4 * hidden_dim]\n",
    "        \n",
    "        # Step 3) Edge logits + probabilities\n",
    "        edge_logits = self.edge_mlp(edge_feat).squeeze(-1)  # [num_edges_dir]\n",
    "        edge_probs = torch.sigmoid(edge_logits)             # [num_edges_dir]\n",
    "        \n",
    "        return GNNSparsifierOutput(\n",
    "            node_embeddings=H,\n",
    "            edge_logits=edge_logits,\n",
    "            edge_probs=edge_probs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a8123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_tensors(G: nx.Graph, device: Optional[torch.device] = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c984924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
